import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
from pydub import AudioSegment
import queue, pydub, tempfile
import whisper
import torch
import torchaudio
import torchvision
from typing import List
import wave
import asyncio
import time
import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
import base64
from gtts import gTTS
import os
import re
from pydub.effects import low_pass_filter, high_pass_filter
from io import BytesIO

def init_page():
    st.set_page_config(
        page_title="Mr.Yas Chatbot(Webã‚«ãƒ¡ã®ç”»åƒã€éŸ³å£°ã‚’è¡¨ç¤º)",
        page_icon="ğŸ¤–"
    )
    st.header("Mr.Yas Chatbot ğŸ¤–")
    st.write("""Webã‚«ãƒ¡ãƒ©ã«ç§»ã—ãŸç”»åƒã«ã¤ã„ã¦ã®å•åˆã›ã€éŸ³å£°ã§ã®å…¥å‡ºåŠ›ãŒã§ãã¾ã™ã€‚\n
             Webãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹è¨­å®šã«ã—ã¦ãã ã•ã„ã€‚""") 

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ] 
 
def select_model():
    #temperature = st.sidebar.slider(
        #"Temperature(å›ç­”ãƒãƒ©ãƒ„ã‚­åº¦åˆ):", min_value=0.0, max_value=2.0, value=0.0, step=0.01)
    temperature = 0.0   
    models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é¸æŠï¼‰:", models)
       
    if model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=512,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_response = llm_response.rstrip()
    print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")
    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response) 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.chat_message("ai"):
        response_placeholder = st.empty()
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
        partial_text = ""
        
        for segment in split_response:
            if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                partial_text += segment
                response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                try:
                    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    cleaned_segment = re.sub(r'[\*!-]', '', segment)
                    tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                    audio_buffer = BytesIO()
                    tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                    audio_buffer.seek(0)

                    # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                    audio = AudioSegment.from_file(audio_buffer, format="mp3")
                    audio = audio._spawn(audio.raw_data, overrides={
                        "frame_rate": int(audio.frame_rate * 1.3)  # 1.5å€é€Ÿ
                    }).set_frame_rate(audio.frame_rate)
                    audio_buffer.close()

                    # éŸ³è³ªèª¿æ•´
                    audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                    audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                    audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                    #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                    low_boost = low_pass_filter(audio,1000).apply_gain(10)
                    audio = audio.overlay(low_boost)

                    # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                    output_buffer = BytesIO()
                    audio.export(output_buffer, format="mp3")
                    output_buffer.seek(0)

                    # éŸ³å£°ã®å†ç”Ÿ
                    # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                    #patterns = ["\n\n1.", "\n\n2.","\n\n3.", "\n\n4.""\n\n5.", "\n\n6.", "\n\n7.","\n\n8.", "\n\n9.""\n\n10.", "\n\n11."]
                    if re.search(r"\n\n", segment):
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                        time.sleep(2) 
                    else:
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    audio_base64 = base64.b64encode(output_buffer.read()).decode()
                    audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
                    a=len(audio_base64)
                    #print(a)
                     # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                    audio_html = f"""
                   <audio id="audio-player" autoplay style="display:none;">
                        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                    </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)

                except Exception as e:
                    #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pass
                try:
                    time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                except Exception as e:
                  time.sleep(2) 


#  LLMå•ç­”é–¢æ•°   
async def query_llm(user_input,frame):
    try:
        if st.session_state.input_img == "æœ‰":    
            # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_image = cv2.imencode('.jpg', frame)[1]
            # ç”»åƒã‚’Base64ã«å¤‰æ›
            base64_image = base64.b64encode(encoded_image).decode('utf-8')  
            #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "keep_gpt-4o":
            llm = st.session_state.llm  
            stream = llm.stream([
                    *st.session_state.message_history,
                    (
                        "user",
                        [
                            {
                                "type": "text",
                                "text": user_input
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "auto"
                                },
                            }
                        ]
                    )
                ])
            #response = chain.invoke(user_input)
         
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            if st.session_state.input_img == "æœ‰":
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }
                            ],
                        ),
                    ]
                )
              
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                #stream = chain.stream(user_input,base64_image)
                stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
                #print("stream=",stream)
                #response = chain.invoke(user_input)
            else:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                }
                            ],
                        ),
                    ]
                )
                
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            if st.session_state.output_method == "éŸ³å£°":
                response = chain.invoke({"user_input":user_input})
                #speak(response)   #st.audio ok
                streaming_text_speak(response)
            else:    
                stream = chain.stream({"user_input":user_input})
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
                with st.chat_message('ai'):  
                    response =st.write_stream(stream) 
                           
            print(f"{st.session_state.model_name}=",response)
 
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
            st.session_state.message_history.append(("user", user_input))
            st.session_state.message_history.append(("ai", response))
        
            return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   

class VideoTransformer(VideoTransformerBase):
        def __init__(self):
            self.frame = None

        def recv(self, frame):   
            self.frame = frame.to_ndarray(format="bgr24")
            return frame
async def process_audio(audio_data_bytes, sample_rate):
    #with wave.open(audio_data_io, 'wb') as wf:
    #with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio_file:    
    temp_audio_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    with wave.open(temp_audio_file, 'wb') as wf:
        wf.setnchannels(2)
        #wf.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã¨ã—ã¦è¨˜éŒ²ã€€NG
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data_bytes)
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ transcribe ã«æ¸¡ã™
    temp_audio_file_path = temp_audio_file.name 
    temp_audio_file.close()  
    # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
    #base:74M,small:244M,medium,large
    # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    try:
        # Whisperã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        result = model.transcribe(temp_audio_file_path, language="ja")  # æ—¥æœ¬èªæŒ‡å®š
        answer = result['text']
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.remove(temp_audio_file_path)
    
        
    # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
    if answer == "" :
        print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
        #return None 
    elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
        print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
        #return None 
    else:
        print(answer)
        return answer
########################################################################### 
def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    """
    OpenAIã®Whisper ASRã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_segment (AudioSegment): æ–‡å­—èµ·ã“ã—ã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã€‚
        debug (bool): Trueã®å ´åˆã€ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
    æˆ»ã‚Šå€¤:
        str: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    if debug:
        save_audio(audio_segment, "debug_audio")
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
        audio = whisper.load_audio(tmpfile.name)
        audio = whisper.pad_or_trim(audio)
        # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
        #base:74M,small:244M,medium,large
        # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        result = model.transcribe(audio, language="ja")  # æ—¥æœ¬èªã‚’æŒ‡å®š
        answer = result['text']
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
        if len(answer) < 5 or "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
            #print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
            return None
        #elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
            #return None 
        
    tmpfile.close()  
    os.remove(tmpfile.name)
    print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
    st.session_state.text_output = answer
    return answer
###############################################################    
def frame_energy(frame):
    # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã¿
    samples = np.frombuffer(frame.to_ndarray().tobytes(), dtype=np.int16)
    # NaNã€æ­£ã®ç„¡é™å¤§ã€è² ã®ç„¡é™å¤§ã‚’0ã«ç½®æ›
    samples = np.nan_to_num(samples, nan=0.0, posinf=0.0, neginf=0.0)
    # é…åˆ—ã®é•·ã•ãŒ0ã®å ´åˆã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’0ã¨ã—ã¦è¿”ã™
    if len(samples) == 0: 
        return 0.0
    # è² ã®å€¤ã‚’çµ¶å¯¾å€¤ã«å¤‰æ›ã—ã¦å‡¦ç† 
    samples = np.abs(samples)
    try:
        #print(np.mean(samples**2))
        energy = np.sqrt(np.mean(samples**2))
        #print("energy=",energy)  #50-90
        return energy  #if not np.isnan(energy) else 0.0 ã“ã‚Œä»˜åŠ ã™ã‚‹ã¨ã ã‚éŸ³å£°ãŒé€”åˆ‡ã‚Œã‚‹
    except Exception as e:
        #print(f"Error exporting audio: {e}")
        return 0.0
        
def frame_amplitude(audio_frame):
    samples = np.frombuffer(audio_frame.to_ndarray().tobytes(), dtype=np.int16)
    max_amplitude = np.max(np.abs(samples))
    #print("max_amplitude=",max_amplitude)
    return max_amplitude 

def process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold, amp_threshold):
    """
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †æ¬¡å‡¦ç†ã—ã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ã§ã™ã€‚
    ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸€å®šæ•°ä»¥ä¸Šç¶šã„ãŸå ´åˆã€ç„¡éŸ³åŒºé–“ã¨ã—ã¦å‡¦ç†ã—ã€å¾Œç¶šã®å‡¦ç†ï¼ˆä¾‹ãˆã°ã€éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã«å½¹ç«‹ã¦ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€ç„¡éŸ³ã‚„éŸ³å£°ã®æœ‰ç„¡ã‚’æ­£ç¢ºã«æ¤œå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frames (list[VideoTransformerBase.Frame]): å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        energy_threshold (int): ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚
        amp_threshold:ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        
    """
        
    for audio_frame in audio_frames:
        sound_chunk = add_frame_to_chunk(audio_frame, sound_chunk)

        energy = frame_energy(audio_frame)
        amplitude = frame_amplitude(audio_frame)

        if energy < energy_threshold or amplitude < amp_threshold:
            silence_frames += 1 
            #ç„¡éŸ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’1ã¤å¢—ã‚„ã—ã¾ã™ã€‚
        else:
            silence_frames = 0 
            #ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦0ã«ã—ã¾ã™ã€‚

    return sound_chunk, silence_frames,energy,amplitude

def add_frame_to_chunk(audio_frame, sound_chunk):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frame (VideoTransformerBase.Frame): è¿½åŠ ã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    æˆ»ã‚Šå€¤ï¼š
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
   
    """
    sound = pydub.AudioSegment(
        data=audio_frame.to_ndarray().tobytes(),
        sample_width=audio_frame.format.bytes,
        frame_rate=audio_frame.sample_rate,
        channels=len(audio_frame.layout.channels),
    )
    sound_chunk += sound
    return sound_chunk

def handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³ã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        silence_frames_threshold (int): ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã—ãã„å€¤ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
   
    """
    if silence_frames >= silence_frames_threshold: 
        #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ100ä»¥ä¸Šã®æ™‚ã€éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆé–“éš”ï¼‰ã¨ã—ã¦æ‰±ã†
        if len(sound_chunk) > 0:
            #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒé€£ç¶šã—ãŸã‚‰ã€éŸ³å£°ã®é€”åˆ‡ã‚Œã¨ã—ã¦ã€ãã“ã¾ã§ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã„ã‚‹
            text = transcribe(sound_chunk)
            #text_output.write(text)
            #print("handle_silenceãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
            #print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³æ™‚ã®å¿œç­”=",text)

            sound_chunk = pydub.AudioSegment.empty()
            silence_frames = 0

    return sound_chunk, silence_frames

def handle_queue_empty(sound_chunk, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    å¼•æ•°:
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤:
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    """
    if len(sound_chunk) > 0:
        text = transcribe(sound_chunk)
        text_output.write(text)
        #print("handle_queue_emptyãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
        #st.session_state.text_output = text
        sound_chunk = pydub.AudioSegment.empty()

    return sound_chunk

def app_sst_with_video():
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã®ãŸã‚ã®ä¸»ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã€‚
        ã“ã®æ©Ÿèƒ½ã¯ã€WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å—ä¿¡ã‚’é–‹å§‹ã—ã€éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã€
        ä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹é™å¯‚ãŒç¶šã„ãŸã¨ãã«éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_receiver_size:å‡¦ç†éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ512
            å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
        status_indicator: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå®Ÿè¡Œä¸­ã¾ãŸã¯åœæ­¢ä¸­ï¼‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        text_output: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        timeout (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå—ä¿¡æ©Ÿã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ç§’ã€‚
        energy_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ•ãƒ¬ãƒ¼ãƒ ãŒé™å¯‚ã¨è¦‹ãªã•ã‚Œã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é–¾å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000ã€‚
        silence_frames_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ–‡å­—èµ·ã“ã—ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®é€£ç¶šã™ã‚‹é™å¯‚ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯100ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    text_input = ""
   
    st.session_state.audio_receiver_size =4096 #2048
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    #with st.sidebar:
    st.header("Webcam Stream")
    webrtc_ctx1 = webrtc_streamer(
        key="example",
        desired_playing_state=True, 
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"video": True, "audio": False},
        video_processor_factory=VideoTransformer,
        )
    #st.sidebar.header("Capture Image") 
    cap_title = st.sidebar.empty()    
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 
    status_indicator = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
    st.sidebar.title("Options")
    init_messages()
    text_output = st.empty()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = "" 
    st.session_state.text_output = ""

    col1, col2 ,col3= st.sidebar.columns(3)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º 
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
      
    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    #print("Before_st.session_state.input_method=",st.session_state.input_method)
    if st.session_state.input_method == "éŸ³å£°": 
        st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
        #status_indicator.write("éŸ³å£°èªè­˜å‹•ä½œä¸­...")
        
        text_output = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ

        audio_receiver_size = st.sidebar.slider(
        "audio_receiver_size(éŸ³å£°å—ä¿¡å®¹é‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4096):", 
        min_value=512, max_value=4096, value=4096, step=512
        )
        # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®é–¾å€¤    
        energy_threshold = st.sidebar.slider(
        "energy_threshold(ç„¡éŸ³æœ€å¤§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ300):", 
        min_value=10, max_value=600, value=300, step=50
        )
        amp_threshold = st.sidebar.slider(
            "amp_threshold(ç„¡éŸ³æœ€å¤§æŒ¯å¹…ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ600):", 
            min_value=0, max_value=1200, value=600, step=50
            )
        silence_frames_threshold = st.sidebar.slider(
            "silence_frames_threshold(é€£ç¶šç„¡éŸ³åŒºé–“ï¼ˆéŸ³å£°é€”åˆ‡ã‚Œãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100):", 
            min_value=0, max_value=200, value=100, step=10
            )
        #60ãŒBest,ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100
        timeout = st.sidebar.slider(
            "timeout(ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ç§’):", 
            min_value=1, max_value=3, value=1, step=1
            )
        
        with st.sidebar:
            webrtc_ctx = webrtc_streamer(
                key="speech-to-text and video",
                desired_playing_state=True, 
                mode=WebRtcMode.SENDONLY,  #SENDRECV, #.
                audio_receiver_size=st.session_state.audio_receiver_size, #audio_receiver_size,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
                #queued_audio_frames_callback=queued_audio_frames_callback,
                rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
                media_stream_constraints={"video": False, "audio": True},
                video_processor_factory=VideoTransformer,
                )
            
        if not webrtc_ctx.state.playing:
            return
        #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
        st.session_state.energy_threshold = energy_threshold
        st.session_state.amp_threshold = amp_threshold
        st.session_state.silence_frames_threshold = silence_frames_threshold
        st.session_state.timeout = timeout

        sound_chunk = pydub.AudioSegment.empty()
        silence_frames = 0

        
        while True:
            if webrtc_ctx.audio_receiver:
                timeout=st.session_state.timeout
                energy_threshold=st.session_state.energy_threshold
                amp_threshold=st.session_state.amp_threshold
                silence_frames_threshold= st.session_state.silence_frames_threshold    

                try:
                    audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=timeout)
                  
                except queue.Empty:
                    status_indicator.write("No frame arrived.")
                    sound_chunk = handle_queue_empty(sound_chunk, text_output)
                    continue
                sound_chunk, silence_frames ,energy,amplitude= process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold,amp_threshold)
                sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output)
                
                try:
                    energy = 0.0 if np.isnan(energy) else energy
                    energy = round(energy)

                except Exception as e:
                    #print(f"Error exporting round(energy): {e}")
                    energy = 0
                status_indicator.write(f"éŸ³å£°ãƒ¬ãƒ™ãƒ«:\n ã‚¨ãƒãƒ«ã‚®ãƒ¼={energy}/threshold={energy_threshold},\n æœ€å¤§æŒ¯å¹…={amplitude}/threshold={amp_threshold}")
            else:    
                status_indicator.write("éŸ³å£°èªè­˜åœæ­¢")

            if len(st.session_state.text_output) > 4 :
                print("st.session_state.text_output=",st.session_state.text_output)    
                text_input =  st.session_state.text_output 
                st.session_state.text_output = ""
            #ã“ã‚Œä»¥é™ã¯ã€éŸ³å£°å…¥åŠ›ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›å…±é€šã®å‡¦ç†ã¸
            if text_input: 
                qa(text_input,webrtc_ctx1,cap_title,cap_image)
                st.write(f"ğŸ¤–ä½•ã‹è©±ã—ã¦!")  
                text_input = ""
                
    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"):
                button_input = "æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("å°æ¾å¸‚ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"):
                button_input = "å°æ¾å¸‚ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        with col8:
            if st.button("ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"):
                button_input = "ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"
        #if button_input !="":
            #st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if button_input:
            text_input = button_input
        if text_input:
            qa(text_input,webrtc_ctx1,cap_title,cap_image)
    ###################################################################################
def qa(text_input,webrtc_ctx1,cap_title,cap_image):
     # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(text_input) - len(text_input.rstrip())
    print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_text = text_input.rstrip()
    print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_text}'")
    with st.chat_message('user'):   
        st.write(cleaned_text) 
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None 
    if st.session_state.input_img == "æœ‰":
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx1.video_transformer: 
            cap = webrtc_ctx1.video_transformer.frame
        if cap is not None :
            #st.sidebar.header("Capture Image") 
            cap_title.header("Capture Image")     
            cap_image.image(cap, channels="BGR")
        else:
            st.warning("WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒã¾ã åˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            
    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.spinner("Querying LLM..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.result= ""
        result = loop.run_until_complete(query_llm(cleaned_text,cap))
        st.session_state.result = result
    result = ""
    text_input=""

################################################################### 
def main():
    #ç”»é¢è¡¨ç¤º
    init_page()
    app_sst_with_video()  
###################################################################      
if __name__ == "__main__":
    main()

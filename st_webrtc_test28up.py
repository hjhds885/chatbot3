import threading
import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
from typing import List
import wave
import asyncio
import time

import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
from langchain_ollama import ChatOllama
import base64
from gtts import gTTS
import os
import whisper
import torch
import torchaudio
import torchvision
import re
import queue, pydub, tempfile
from pydub import AudioSegment
from pydub.effects import low_pass_filter, high_pass_filter
from io import BytesIO
import psutil
import gc
from scipy.signal import resample

# é–¢æ•°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
def get_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§è¿”ã™
    return mem_info.rss / (1024 * 1024)

def current_memory_use(memory_use,memory_alt,memory_ok):
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    current_memory_usage = get_memory_usage()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º
    
    memory_use.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    #print("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å®šç¾©
    MEMORY_LIMIT_MB = 2700  # 1GB
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
    if current_memory_usage > MEMORY_LIMIT_MB:
      
        memory_alt.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
        #st.stop()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
    else:
        
        memory_ok.success("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")
        #print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")


def init_page():
    st.set_page_config(
        page_title="Yas Chatbot(Webã‚«ãƒ¡ã®ç”»åƒã€éŸ³å£°ã‚’è¡¨ç¤º)",
        page_icon="ğŸ¤–"
    )
    st.header("Yas Chatbot ğŸ¤–")
    st.write("""Webã‚«ãƒ¡ãƒ©ã«ç§»ã—ãŸç”»åƒã«ã¤ã„ã¦ã®å•åˆã›ã€éŸ³å£°ã§ã®å…¥å‡ºåŠ›ãŒã§ãã¾ã™ã€‚\n
             Webãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹è¨­å®šã«ã—ã¦ãã ã•ã„ã€‚""") 
    st.sidebar.title("Options")

def init_messages():
    clear_button = st.sidebar.button("ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]   
         
def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    temperature = 0.0
    #models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    #model = st.sidebar.radio("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:", models)
    model = st.sidebar.selectbox(
        "LLMå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        ["GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro"]
    )
    if model == "llava-llama3":  
        st.session_state.model_name = "llava-llama3"
        return ChatOllama(
            temperature=temperature,
            model=st.session_state.model_name,
            #api_key= st.secrets.key.OPENAI_API_KEY,
            #streaming=True,
        )      
    elif model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=512,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            #model=st.session_state.model_name,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
async def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    #trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    #print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    #cleaned_response = llm_response.rstrip()
    #print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")
    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response)
    # ãƒ¡ãƒ¢ãƒªè§£æ”¾
    del  llm_response
    gc.collect() 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.chat_message("ai"):
        response_placeholder = st.empty()
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
        partial_text = ""
        
        for segment in split_response:
            if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                partial_text += segment
                response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                try:
                    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    cleaned_segment = re.sub(r'[\*#*!-]', '', segment)
                    tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                    # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                    del  cleaned_segment
                    gc.collect() 
                    #audio_buffer = BytesIO()
                    with BytesIO() as audio_buffer:
                        tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                        audio_buffer.seek(0)

                    # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                    audio = AudioSegment.from_file(audio_buffer, format="mp3")
                    audio = audio._spawn(audio.raw_data, overrides={
                        "frame_rate": int(audio.frame_rate * 1.3)  # 1.5å€é€Ÿ
                    }).set_frame_rate(audio.frame_rate)
                    #audio_buffer.close()

                    # éŸ³è³ªèª¿æ•´
                    #audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                    #audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                    #audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                    #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                    #low_boost = low_pass_filter(audio,1000).apply_gain(10)
                    #audio = audio.overlay(low_boost)

                    # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                     #output_buffer = BytesIO()
                    with BytesIO() as output_buffer:
                        audio.export(output_buffer, format="mp3")
                        output_buffer.seek(0)
                        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                        del tts, audio 
                        gc.collect() 
    
                        # éŸ³å£°ã®å†ç”Ÿ
                        # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                        #if re.search(r"\n\n", segment):
                            #print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                            #time.sleep(1) 
                        #else:
                            #print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                        #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        audio_base64 = base64.b64encode(output_buffer.read()).decode()
                        audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
                        a=len(audio_base64)
                        #print(a)
                         # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                        audio_html = f"""
                       <audio id="audio-player" autoplay style="display:none;">
                            <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                        </audio>
                        """
                        st.markdown(audio_html, unsafe_allow_html=True)

                except Exception as e:
                    #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pass
                try:
                    time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                except Exception as e:
                  time.sleep(2)
        # æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªè§£æ”¾
        del split_response, partial_text,audio_buffer,audio_base64
        gc.collect()

#  LLMå•ç­”é–¢æ•°
async def query_llm(user_input,frame):
    if st.session_state.model_name ==  "llava-llama3":
        user_input = " æ¬¡ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚" + user_input 
    try:
        if st.session_state.input_img == "æœ‰":    
            # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_image = cv2.imencode('.jpg', frame)[1]
            # ç”»åƒã‚’Base64ã«å¤‰æ›
            base64_image = base64.b64encode(encoded_image).decode('utf-8')  
            #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            if st.session_state.input_img == "æœ‰":
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }
                            ],
                        ),
                    ]
                )
              
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                #stream = chain.stream(user_input,base64_image)
                stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
                #print("stream=",stream)
                #response = chain.invoke(user_input)
            else:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                }
                            ],
                        ),
                    ]
                )
                
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            if st.session_state.output_method == "éŸ³å£°":
                response = chain.invoke({"user_input":user_input})
                #speak(response)   #st.audio ok
                await streaming_text_speak(response)
            else:    
                stream = chain.stream({"user_input":user_input})
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
                with st.chat_message('ai'):  
                    response =st.write_stream(stream) 
                           
            print(f"{st.session_state.model_name}=",response)
 
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
            st.session_state.message_history.append(("user", user_input))
            st.session_state.message_history.append(("ai", response))
        
            return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   
    # ãƒ¡ãƒ¢ãƒªè§£æ”¾
    del user_input, base64_image, frame,response,stream,chain,prompt,output_parser
    gc.collect() 
    
def main():
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
    init_messages()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = st.session_state.get("input_method", "")
    st.session_state.user_input = st.session_state.get("user_input", "")
    st.session_state.result = st.session_state.get("result", "")
    st.session_state.frame = st.session_state.get("frame", "")
    col1, col2 ,col3= st.sidebar.columns(3)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º (ç¬¬2ç« ã‹ã‚‰å°‘ã—ä½ç½®ãŒå¤‰æ›´ã«ãªã£ã¦ã„ã‚‹ã®ã§æ³¨æ„)
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
    #ãƒ‡ãƒ¼ã‚¿åˆæœŸå€¤
    #user_input = ""
    #base64_image = ""
    #frame = ""    
    app_sst_with_video() 

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame

async def process_audio(audio_data_bytes, sample_rate):
    temp_audio_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    with wave.open(temp_audio_file, 'wb') as wf:
        wf.setnchannels(2)
        #wf.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã¨ã—ã¦è¨˜éŒ²ã€€NG
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data_bytes)
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ transcribe ã«æ¸¡ã™
    temp_audio_file_path = temp_audio_file.name 
    temp_audio_file.close()  
    # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    #ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’1ã¤ã ã‘ä¿æŒã—ã€ç¹°ã‚Šè¿”ã—åˆ©ç”¨    
    if "whisper_model" not in st.session_state:
        st.session_state.whisper_model = whisper.load_model("small")
    whisper_model = st.session_state.whisper_model  
    # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ, weights_only=True # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
    #base:74M,small:244M,medium,large
    # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    try:
        # Whisperã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        result = whisper_model.transcribe(temp_audio_file_path, language="ja")  # æ—¥æœ¬èªæŒ‡å®š
        answer = result['text']
        result = ""
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.remove(temp_audio_file_path)
    
        
    # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
    if len(answer) < 5 or "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
            #print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
            return ""
    elif "è¦‹ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†" in answer or "ã¯ã£ã¯ã£ã¯" in answer:
        #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
        return "" 
    elif "ã‚“ã‚“ã‚“ã‚“ã‚“ã‚“" in answer or "ã‚¹ã‚¿ãƒƒãƒ•ã•ã‚“ã®ãŠã‹ã’ã§" in answer:
        #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
        return "" 
    else:
        print("answer=",answer)
        st.session_state.user_input = answer
        return answer

def app_sst_with_video():
    memory_use = st.sidebar.empty()
    memory_alt = st.sidebar.empty()
    memory_ok = st.sidebar.empty()
    current_memory_use(memory_use,memory_alt,memory_ok)
    
    text_input = ""

    # deque ã®æœ€å¤§é•·ã‚’è¨­å®š# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›ã‚’ç‹™ã†
    # å¿…è¦æœ€å°é™ã®éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ä¿æŒã—ã¾ã™ã€‚
    frames_deque = deque(maxlen=5)  # å¿…è¦ã«å¿œã˜ã¦æœ€å¤§é•·ã‚’èª¿æ•´
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])

    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)

        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)

        return new_frames
        
     # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    if "streaming" not in st.session_state:
        st.session_state["streaming"] = True  # åˆæœŸçŠ¶æ…‹ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿä¸­
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=st.session_state["streaming"], 
            mode=WebRtcMode.SENDRECV, #.SENDONLY,  #
            #audio_receiver_size=2048,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            #å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  
        )
    
    if not webrtc_ctx.state.playing:
        return
    #status_indicator.write("Loading...")
    cap_title = st.sidebar.empty()    
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 
    #status_indicator = st.sidebar.empty()
    current_memory_use(memory_use,memory_alt,memory_ok)
    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    #print("Before_st.session_state.input_method=",st.session_state.input_method)
    if st.session_state.input_method == "éŸ³å£°":
        #st.session_state["streaming"] = False  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœæ­¢
        st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦! ....  éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚") 
        status_indicator = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        #status_indicator.write("éŸ³å£°èªè­˜å‹•ä½œä¸­...")
        text_output = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        
        st.sidebar.header("Capture Image")
        cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        audio_buffer = []  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚çš„ã«æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ 
        i=0       
        while True:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
            current_memory_use(memory_use,memory_alt,memory_ok)
                # deque ã®æœ€å¤§é•·ã‚’è¨­å®š
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
            mem_use= get_memory_usage() 
            i = i + 1
            print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={i}å›ç›®:{mem_use}")
            #audio =np.nan_to_num(audio_frame, nan=0.0, posinf=0.0, neginf=0.0)
            
            if webrtc_ctx.state.playing:
                audio_frames = []
                with frames_deque_lock:
                    while len(frames_deque) > 0:
                        frame = frames_deque.popleft() #å¤ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‰Šé™¤
                        audio_frames.append(frame)
                #ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã§ããªã‹ã£ãŸæ™‚
                if len(audio_frames) == 0:
                    time.sleep(0.1)
                    #status_indicator.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
                    continue
                print("len(audio_frames)=",len(audio_frames))
                audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’åˆæœŸåŒ–
                for audio_frame in audio_frames:
                    
                    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ numpy é…åˆ—ã¨ã—ã¦å–å¾—ï¼ˆs16 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ int16 ã¨ã—ã¦è§£é‡ˆï¼‰
                    #audio = audio_frame.to_ndarray().astype(np.int16)
                    #audio_buffer.append(audio)  # ãƒãƒƒãƒ•ã‚¡ã«ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    audio_buffer.append(audio_frame.to_ndarray().astype(np.int16)) 
                    # æ­£è¦åŒ–ã—ã¦ -1.0 ã‹ã‚‰ 1.0 ã®ç¯„å›²ã«åã‚ã‚‹
                    #max_val = np.max(np.abs(audio_buffer))
                    #if max_val > 0:
                        #audio_buffer = audio_buffer / max_val

                if len(audio_buffer) >0:  # 100: # 
                    # è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã¾ã¨ã‚ã‚‹ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã—ã¦ 1 ã¤ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«
                    audio_data = np.concatenate(audio_buffer)
                    audio_data_bytes= audio_data.tobytes()
                    st.session_state.user_input=""
                    # éåŒæœŸã§éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
                    result=asyncio.run(process_audio(audio_data_bytes, frame.sample_rate))
                    
                    # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                    audio_buffer.clear() 
                    # ãƒ¡ãƒ¢ãƒªã®é–‹æ”¾
                    audio_frames = []
                    del audio_frames
                    del audio_buffer[:]
                    del audio_data
                    del audio_data_bytes
                    gc.collect() 
                    
                    #print("ã“ã“ã‚’é€šéE2") #ã“ã“ã¾ã§OK
                    #print("text_input=",text_input)
                    if len(st.session_state.user_input) > 4 :
                        print("st.session_state.user_input=",st.session_state.user_input)  
                        text_input = st.session_state.user_input
                        #st.write("text_input:",text_input)
                        #ã“ã‚Œä»¥é™ã¯ã€éŸ³å£°å…¥åŠ›ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›å…±é€šã®å‡¦ç†ã¸
                        qa(text_input,webrtc_ctx,cap_title,cap_image)
                        st.write(f"ğŸ¤–ä½•ã‹è©±ã—ã¦!")
                        #status_indicator.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")  
                        text_input = ""       
            else:
                #status_indicator.write("ğŸ¤–éŸ³å£°èªè­˜ãŒåœæ­¢ã—ã¦ã„ã¾ã™ã€‚")
                st.write("ğŸ¤–éŸ³å£°èªè­˜ãŒåœæ­¢ã—ã¦ã„ã¾ã™ã€‚Webãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
                break

    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        st.session_state["streaming"] = True  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿ
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"):
                button_input = "æ—¥æœ¬ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("å°æ¾å¸‚ã®ãŠã„ã—ã„æ–™ç†åº—ã¯ï¼Ÿ"):
                button_input = "å°æ¾å¸‚ã®ãŠã„ã—ã„æ–™ç†åº—ã¯ï¼Ÿ"
        with col8:
            if st.button("ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"):
                button_input = "ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"
        if button_input !="":
            st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if button_input:
            text_input = button_input
        if text_input:
            qa(text_input,webrtc_ctx,cap_title,cap_image)
            text_input = ""
            
def qa(text_input,webrtc_ctx,cap_title,cap_image):
     # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(text_input) - len(text_input.rstrip())
    print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_text = text_input.rstrip()
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_text}'")
    with st.chat_message('user'):   
        st.write(cleaned_text) 
        cleaned_text = ""
    
    
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None 
    if st.session_state.input_img == "æœ‰":
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx.video_transformer:  
            cap = webrtc_ctx.video_transformer.frame
        if cap is not None :
            #st.sidebar.header("Capture Image") 
            cap_title.header("Capture Image")     
            cap_image.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.spinner("Querying LLM..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.result= ""
        result = loop.run_until_complete(query_llm(cleaned_text,cap))
        st.session_state.result = result
    result = ""
    text_input="" 
    cap = None
    del cap
    gc.collect() 

###################################################################      
if __name__ == "__main__":
    main()

import threading
import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase

from pydub import AudioSegment
import queue, pydub, tempfile
import whisper
import torch
import torchaudio
import torchvision

from typing import List
import wave
import asyncio
import time

import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
import base64
from gtts import gTTS
import os
import re

def init_page():
    st.set_page_config(
        page_title="Mr.Yas Chatbot",
        page_icon="ğŸ¤–"
    )
    st.header("Mr.Yas Chatbot ğŸ¤–")
    st.write("""Safari,Chrome,Firefoxãªã©Webãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹è¨­å®šã«ã—ã¦ãã ã•ã„ã€‚
         support.apple.com,support.google.com,support.mozilla.orgå‚ç…§ã€‚""") 
    

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ] 
    

def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    #temperature = st.sidebar.slider(
        #"Temperature(å›ç­”ãƒãƒ©ãƒ„ã‚­åº¦åˆ):", min_value=0.0, max_value=2.0, value=0.0, step=0.01)
    temperature = 0.0   
    models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("Choose a modelï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼‰:", models)
       
    if model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=512,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            #model=st.session_state.model_name,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
def speak(text):
    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    cleaned_text = re.sub(r'\*!?', '', text)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›
    tts = gTTS(text=cleaned_text, lang='ja')

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
        tts.save(temp_file.name)
        temp_file_path = temp_file.name

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
    audio = AudioSegment.from_file(temp_file_path)
    # é€Ÿåº¦ã‚’1.5å€ã«ã™ã‚‹ï¼ˆ2.0ã«ã™ã‚‹ã¨2å€é€Ÿï¼‰
    faster_audio = audio.speedup(playback_speed=1.5)

    # ã‚‚ã†ä¸€ã¤ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as faster_temp_file:
        faster_audio.export(faster_temp_file.name, format="mp3")
        faster_temp_file_path = faster_temp_file.name

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›
    with open(faster_temp_file_path, "rb") as audio_file:
        audio_bytes = audio_file.read()
        st.audio(audio_bytes, format="audio/mp3", start_time=0, autoplay=True)

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(temp_file_path)
    os.remove(faster_temp_file_path)

#  LLMå•ç­”é–¢æ•°   
async def query_llm(user_input,frame):
    print("user_input=",user_input)
    
    try:
        if st.session_state.input_img == "æœ‰":    
            # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_image = cv2.imencode('.jpg', frame)[1]
            # ç”»åƒã‚’Base64ã«å¤‰æ›
            base64_image = base64.b64encode(encoded_image).decode('utf-8')  
            #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "keep_gpt-4o":
            llm = st.session_state.llm  
            stream = llm.stream([
                    *st.session_state.message_history,
                    (
                        "user",
                        [
                            {
                                "type": "text",
                                "text": user_input
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "auto"
                                },
                            }
                        ]
                    )
                ])
            #response = chain.invoke(user_input)
           
        
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            
        elif st.session_state.model_name ==  "keep_command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            prompt = ChatPromptTemplate.from_messages([
                    *st.session_state.message_history,
                    ("user", "{user_input}")  # ã“ã“ã«ã‚ã¨ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒå…¥ã‚‹
                ])
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            stream = chain.stream(user_input)
            
            #response = chain.invoke(user_input)
            
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            if st.session_state.input_img == "æœ‰":
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }
                            ],
                        ),
                    ]
                )
              
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                #stream = chain.stream(user_input,base64_image)
                stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
                #print("stream=",stream)
                #response = chain.invoke(user_input)
            else:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                }
                            ],
                        ),
                    ]
                )
                
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                stream = chain.stream({"user_input":user_input})
        # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
        #llm_output = st.empty()
        with st.chat_message('ai'):   #llm_output.chat_message('ai'):
            #st.write(response)  
            response =st.write_stream(stream) 
        #print("response=",response)            
        print(f"{st.session_state.model_name}=",response)
 
        # éŸ³å£°å‡ºåŠ›å‡¦ç†                
        if st.session_state.output_method == "éŸ³å£°":
            #st.write("éŸ³å£°å‡ºåŠ›ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            speak(response)   #st.audio ok
            #speak1(response) pygame NG
            #speak_thread = speak_async(response)
            # å¿…è¦ã«å¿œã˜ã¦éŸ³å£°åˆæˆã®å®Œäº†ã‚’å¾…ã¤
            #speak_thread.join() 
            #print("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ¬¡ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            #st.write("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ¬¡ã®å‡¦ç†ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚")
            
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))
    
        return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   



class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame
async def process_audio(audio_data_bytes, sample_rate):
    #with wave.open(audio_data_io, 'wb') as wf:
    #with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio_file:    
    temp_audio_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    with wave.open(temp_audio_file, 'wb') as wf:
        wf.setnchannels(2)
        #wf.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã¨ã—ã¦è¨˜éŒ²ã€€NG
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data_bytes)
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ transcribe ã«æ¸¡ã™
    temp_audio_file_path = temp_audio_file.name 
    temp_audio_file.close()  
    # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
    #base:74M,small:244M,medium,large
    # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    try:
        # Whisperã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        result = model.transcribe(temp_audio_file_path, language="ja")  # æ—¥æœ¬èªæŒ‡å®š
        answer = result['text']
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.remove(temp_audio_file_path)
    
        
    # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
    if answer == "" :
        print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
        #return None 
    elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
        print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
        #return None 
    else:
        print(answer)
        return answer
###########################################################################    
########################################################################### 
def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    """
    OpenAIã®Whisper ASRã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_segment (AudioSegment): æ–‡å­—èµ·ã“ã—ã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã€‚
        debug (bool): Trueã®å ´åˆã€ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
    æˆ»ã‚Šå€¤:
        str: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    if debug:
        save_audio(audio_segment, "debug_audio")
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")

        ##########################################################
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
        audio = whisper.load_audio(tmpfile.name)
        audio = whisper.pad_or_trim(audio)
        # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
        #base:74M,small:244M,medium,large
        # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        result = model.transcribe(audio, language="ja")  # æ—¥æœ¬èªã‚’æŒ‡å®š
        answer = result['text']
      
        # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
        if answer == "" :
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
            return None 
        elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
            return None 
        #else:
            #print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
            #st.session_state.text_output = answer
            #return answer
        
        ############################################################
    tmpfile.close()  
    os.remove(tmpfile.name)
    print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
    st.session_state.text_output = answer

    return answer
###############################################################    
def frame_energy(frame):
    samples = np.frombuffer(frame.to_ndarray().tobytes(), dtype=np.int16)
    #################################################################
    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚µãƒ³ãƒ—ãƒ«ã®ä¸€éƒ¨ã‚’å‡ºåŠ› 
    #print("Samples:", samples[:10])
    # NaNã‚„ç„¡é™å¤§ã®å€¤ã‚’é™¤å» 
    #if not np.isfinite(samples).all(): 
        #samples = samples[np.isfinite(samples)]
    #np.isfinite() ã§ç„¡åŠ¹ãªå€¤ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã ã‘ã§ã¯ã€
    # ç©ºé…åˆ—ã®ã‚¨ãƒ©ãƒ¼ãŒå†ã³ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
    # np.nan_to_num ã‚’ä½¿ç”¨ã—ãŸã»ã†ãŒå®‰å…¨ã«å‡¦ç†ã§ãã¾ã™ã€‚
    # ç„¡åŠ¹ãªå€¤ã‚’å®‰å…¨ãªå€¤ã«ç½®æ›
    samples = np.nan_to_num(samples, nan=0.0, posinf=0.0, neginf=0.0)
    ##################################################################
    if len(samples) == 0: 
        return 0.0
    energy = np.sqrt(np.mean(samples**2)) 
    #print("Energy:", energy) 
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å‡ºåŠ› 
    return energy
###########################################################################
def is_silent_frame(audio_frame, amp_threshold):
    """
    ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³ã‹ã©ã†ã‹ã‚’æœ€å¤§æŒ¯å¹…ã§åˆ¤å®šã™ã‚‹é–¢æ•°ã€‚
    """
    samples = np.frombuffer(audio_frame.to_ndarray().tobytes(), dtype=np.int16)
    max_amplitude = np.max(np.abs(samples))
    return max_amplitude < amp_threshold

def process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold, amp_threshold):
    """
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †æ¬¡å‡¦ç†ã—ã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ã§ã™ã€‚
    ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸€å®šæ•°ä»¥ä¸Šç¶šã„ãŸå ´åˆã€ç„¡éŸ³åŒºé–“ã¨ã—ã¦å‡¦ç†ã—ã€å¾Œç¶šã®å‡¦ç†ï¼ˆä¾‹ãˆã°ã€éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã«å½¹ç«‹ã¦ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€ç„¡éŸ³ã‚„éŸ³å£°ã®æœ‰ç„¡ã‚’æ­£ç¢ºã«æ¤œå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frames (list[VideoTransformerBase.Frame]): å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        energy_threshold (int): ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚
        amp_threshold:ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        
    """
        
    for audio_frame in audio_frames:
        sound_chunk = add_frame_to_chunk(audio_frame, sound_chunk)

        energy = frame_energy(audio_frame)
        
        if energy < energy_threshold or is_silent_frame(audio_frame, amp_threshold):
            silence_frames += 1 
            #ç„¡éŸ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’1ã¤å¢—ã‚„ã—ã¾ã™ã€‚
        else:
            silence_frames = 0 
            #ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦0ã«ã—ã¾ã™ã€‚

    return sound_chunk, silence_frames

def add_frame_to_chunk(audio_frame, sound_chunk):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frame (VideoTransformerBase.Frame): è¿½åŠ ã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    æˆ»ã‚Šå€¤ï¼š
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
   
    """
    sound = pydub.AudioSegment(
        data=audio_frame.to_ndarray().tobytes(),
        sample_width=audio_frame.format.bytes,
        frame_rate=audio_frame.sample_rate,
        channels=len(audio_frame.layout.channels),
    )
    sound_chunk += sound
    return sound_chunk

def handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³ã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        silence_frames_threshold (int): ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã—ãã„å€¤ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
   
    """
    if silence_frames >= silence_frames_threshold: 
        #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ100ä»¥ä¸Šã®æ™‚ã€éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆé–“éš”ï¼‰ã¨ã—ã¦æ‰±ã†
        if len(sound_chunk) > 0:
            text = transcribe(sound_chunk)
            text_output.write(text)
            #print("handle_silenceãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
            #print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³æ™‚ã®å¿œç­”=",text)
            sound_chunk = pydub.AudioSegment.empty()
            silence_frames = 0

    return sound_chunk, silence_frames

def handle_queue_empty(sound_chunk, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    å¼•æ•°:
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤:
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    """
    if len(sound_chunk) > 0:
        text = transcribe(sound_chunk)
        text_output.write(text)
        #print("handle_queue_emptyãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
        #st.session_state.text_output = text
        sound_chunk = pydub.AudioSegment.empty()

    return sound_chunk

def app_sst_with_video():
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã®ãŸã‚ã®ä¸»ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã€‚
        ã“ã®æ©Ÿèƒ½ã¯ã€WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å—ä¿¡ã‚’é–‹å§‹ã—ã€éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã€
        ä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹é™å¯‚ãŒç¶šã„ãŸã¨ãã«éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_receiver_size:å‡¦ç†éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ512
            å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
        status_indicator: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå®Ÿè¡Œä¸­ã¾ãŸã¯åœæ­¢ä¸­ï¼‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        text_output: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        timeout (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå—ä¿¡æ©Ÿã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ç§’ã€‚
        energy_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ•ãƒ¬ãƒ¼ãƒ ãŒé™å¯‚ã¨è¦‹ãªã•ã‚Œã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é–¾å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000ã€‚
        silence_frames_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ–‡å­—èµ·ã“ã—ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®é€£ç¶šã™ã‚‹é™å¯‚ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯100ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    text_input = ""
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])
    #è¤‡æ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒæ™‚ã«å…±æœ‰ãƒªã‚½ãƒ¼ã‚¹ã‚’æ“ä½œã™ã‚‹ã“ã¨ã§ã€ç«¶åˆçŠ¶æ…‹ãŒç™ºç”Ÿã—ã¾ã™ã€‚
    #ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«ã™ã‚‹ã«ã¯ã€ãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹éš›ã«æ’ä»–åˆ¶å¾¡ï¼ˆmutexã‚„lockï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    #Pythonã®queue.Queueãªã©ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)
        # å—ä¿¡ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å‡¦ç†ã™ã‚‹ãŸã‚ã€
        # frames_deque_lockã§ãƒ­ãƒƒã‚¯ã‚’ã‹ã‘ãªãŒã‚‰
        # frames_dequeï¼ˆã‚­ãƒ¥ãƒ¼ã®ã‚ˆã†ãªæ§‹é€ ï¼‰ã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è“„ç©ã—ã¾ã™ã€‚
        # ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€åˆ¥ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é€æ¬¡å–ã‚Šå‡ºã—ã¦éŸ³å£°èªè­˜å‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªframes_dequeã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚

        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            #æ–°ã—ã„ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’av.AudioFrame.from_ndarrayã‚’ç”¨ã„ã¦ç”Ÿæˆã—ã€
            # åŒã˜ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)

        return new_frames
        #æ¶ˆéŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¿”å´
        #å‡¦ç†å¾Œã®ç„¡éŸ³åŒ–ã•ã‚ŒãŸã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚¹ãƒˆnew_framesã«è¿½åŠ ã—ã€ãã‚Œã‚’è¿”å´ã—ã¾ã™ã€‚
        #æ¶ˆéŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™ç†ç”±ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®éŸ³å£°ã‚’ãã®å ´ã§å‡ºåŠ›ã—ãªã„ï¼ˆé€ä¿¡ã—ãªã„ï¼‰ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚
        # ä»£ã‚ã‚Šã«å‡¦ç†ãŒé€²ã‚€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    st.session_state.audio_receiver_size =2048
    #audio_receiver_size = 2048

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text and video",
            desired_playing_state=True, 
            mode=WebRtcMode.SENDRECV,  #SENDRECV, #.SENDONLY
            audio_receiver_size=st.session_state.audio_receiver_size, #2048, #audio_receiver_size,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  #æ©Ÿèƒ½ã—ã¦ã„ã‚‹ï¼Ÿ
            )
        
    st.sidebar.header("Capture Image")    
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 
    
    st.sidebar.title("Options")
    init_messages()
    status_indicator = st.empty()
    #usr_input = st.empty()
    text_output = st.empty()
    #llm_output = st.empty()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = "" 
    st.session_state.text_output = ""
    col1, col2 ,col3= st.sidebar.columns(3)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º 
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
    #ãƒ‡ãƒ¼ã‚¿åˆæœŸå€¤
    user_input = ""
    base64_image = ""
    frame = ""  

    if not webrtc_ctx.state.playing:
        return
    #status_indicator.write("Loading...")

    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    #print("Before_st.session_state.input_method=",st.session_state.input_method)
    if st.session_state.input_method == "éŸ³å£°": 
        audio_receiver_size = st.sidebar.slider(
            "audio_receiver_size(å‡¦ç†éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1024):", 
            min_value=64, max_value=2048, value=1024, step=64
            )
        st.session_state.audio_receiver_size = audio_receiver_size
        energy_threshold = st.sidebar.slider(
        "energy_threshold(ç„¡éŸ³ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2000):", 
        min_value=100, max_value=5000, value=2000, step=100
        )
        amp_threshold = st.sidebar.slider(
            "amp_threshold(ç„¡éŸ³æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.3):", 
            min_value=0.00, max_value=1.00, value=0.30, step=0.05
            )
        # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®é–¾å€¤ 0.01 0.05 1.00ä»¥ä¸‹
            #amp_threshold = 0.30  #0.05
        silence_frames_threshold = st.sidebar.slider(
            "silence_frames_threshold(ãƒˆãƒªã‚¬ãƒ¼ç”¨é€£ç¶šç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100):", 
            min_value=20, max_value=300, value=60, step=20
            )
        #60ãŒBest,ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100
        #timeout = st.sidebar.slider(
            #"timeout(ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ç§’):", 
            #min_value=1, max_value=3, value=1, step=1
            #)
        #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
        #st.session_state.energy_threshold = energy_threshold
        #st.session_state.amp_threshold = amp_threshold
        #st.session_state.silence_frames_threshold = silence_frames_threshold
        #st.session_state.timeout = timeout
      
        sound_chunk = pydub.AudioSegment.empty()
        #éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©ã™ã‚‹ãŸã‚ã®pydub.AudioSegmentã®ç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦åˆæœŸåŒ–
        silence_frames = 0

        st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
        status_indicator = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        status_indicator.write("Loading...")
        text_output = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        
        while True:
            #if webrtc_ctx.audio_receiver:
            if webrtc_ctx.state.playing:
                #timeout=st.session_state.timeout
                #energy_threshold=st.session_state.energy_threshold
                #amp_threshold=st.session_state.amp_threshold
                #silence_frames_threshold= st.session_state.silence_frames_threshold 
                #print("ã“ã“ã‚’é€šéA") #ã“ã“ã¾ã§OK 

                #ã‚ªãƒªã‚¸ãƒŠãƒ«ã®éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªframes_dequeã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã€‚
                #frames_deque_lockã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«æ“ä½œã—ã¦ã„ã¾ã™ã€‚
                audio_frames = []
                with frames_deque_lock:
                    while len(frames_deque) > 0:
                        frame = frames_deque.popleft()
                        audio_frames.append(frame)
                #audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=timeout)
                    # ã‚­ãƒ¥ãƒ¼ã‚’å®šæœŸçš„ã«ã‚¯ãƒªã‚¢timeout
                #audio_frames = audio_frames 
                #except queue.Empty:
                # ãƒ•ãƒ¬ãƒ¼ãƒ æœªåˆ°é”æ™‚ã®å‡¦ç†
            # ãƒ•ãƒ¬ãƒ¼ãƒ æœªåˆ°é”æ™‚ã®å‡¦ç†
                if not audio_frames:
                    status_indicator.write("No frame arrived.")
                    sound_chunk = handle_queue_empty(sound_chunk, text_output)
                    continue
                #except Exception as e: 
                #    print(f"Error while clearing audio queue: {e}") 
                #    time.sleep(1) # 1ç§’ã”ã¨ã«ã‚¯ãƒªã‚¢,å¿…è¦ã«å¿œã˜ã¦èª¿æ•´

                #é™å¯‚ã®ã‚«ã‚¦ãƒ³ãƒˆã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤åˆ¤å®š    
                sound_chunk, silence_frames = process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold,amp_threshold)
                #ä¸€å®šã®é™å¯‚ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®å‡¦ç†ï¼ˆä¾‹: éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã‚’å®Ÿè¡Œ
                sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output)
                
            else:
                #ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒåœæ­¢ã—ãŸï¼ˆwebrtc_ctx.state.playing == Falseï¼‰å ´åˆã€
                # è“„ç©ã•ã‚ŒãŸéŸ³å£°ï¼ˆsound_chunkï¼‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚
                # transcribeé–¢æ•°ã§éŸ³å£°èªè­˜ã‚’è¡Œã„ã¾ã™ã€‚
                status_indicator.write("Stopping.")
                if len(sound_chunk) > 0:
                    #print("len(sound_chunk)=",len(sound_chunk))
                    try:
                        text = transcribe(sound_chunk.raw_data)
                        text_output.write(text)
                        print("else_Stoppingãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
                    except Exception as e:
                        text_output.write("Error during transcription.")
                        print(f"Error during transcription: {e}")
                                        
                    print("ã“ã“ã‚’é€šéE2") #ã“ã“ã¾ã§OK
                break
                       
            st.session_state.user_input=st.session_state.text_output
            if st.session_state.user_input != "":    
                print("user_input=",st.session_state.user_input)
            
                with st.chat_message('user'):   
                    st.write(st.session_state.user_input)

                cap = None 
                if st.session_state.input_img == "æœ‰":
                    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
                    #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
                    #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
                    if webrtc_ctx.video_transformer:  
                        cap = webrtc_ctx.video_transformer.frame
                    if cap is not None :
                        #st.sidebar.header("Capture Image")
                        cap_image.image(cap, channels="BGR")
                        # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                
                with st.spinner("Querying LLM..."):
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    st.session_state.result= ""
                    result = loop.run_until_complete(query_llm(st.session_state.user_input,cap))
                    st.session_state.result = result
                    st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
                result = ""
                st.session_state.text_output=""
                st.session_state.user_input=""
    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"):
                button_input = "æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("æ—¥æœ¬ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"):
                button_input = "æ—¥æœ¬ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        with col8:
            if st.button("ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"):
                button_input = "ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"
        if button_input !="":
            st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if text_input:
            st.session_state.user_input=text_input
            text_input=""
            #llm_in()
        if st.session_state.user_input != "":    
            print("user_input=",st.session_state.user_input)
            with st.chat_message('user'):   
                st.write(st.session_state.user_input) 
            # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
            cap = None 
            if st.session_state.input_img == "æœ‰":
                # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
                #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
                #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
                if webrtc_ctx.video_transformer:  
                    cap = webrtc_ctx.video_transformer.frame
                if cap is not None :
                    #st.sidebar.header("Capture Image")
                    cap_image.image(cap, channels="BGR")
                    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                    
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
            with st.spinner("Querying LLM..."):
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                st.session_state.result= ""
                result = loop.run_until_complete(query_llm(st.session_state.user_input,cap))
                st.session_state.result = result
                result = ""
                st.session_state.user_input=""

################################################################### 
def main():
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
   
     
    app_sst_with_video()
        #webcam_placeholder,
        #webrtc_ctx,
        #status_indicator,
        #text_output,
        #cap_image,
        #audio_receiver_size,
        #timeout,
        #energy_threshold,
        #amp_threshold,
        #silence_frames_threshold,
        #)  
###################################################################      
if __name__ == "__main__":
    main()

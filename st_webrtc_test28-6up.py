import threading
import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
from typing import List,Tuple
import wave
import asyncio
import time
from time import sleep
import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
from langchain_ollama import ChatOllama
import base64
from gtts import gTTS
import os
import whisper
import torch
import torchaudio
import torchvision
import re
import queue, pydub, tempfile
from pydub import AudioSegment
from pydub.effects import low_pass_filter, high_pass_filter
from io import BytesIO
import psutil
import gc
from scipy.signal import resample
import librosa
import subprocess

# é–¢æ•°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
def get_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§è¿”ã™
    return mem_info.rss / (1024 * 1024)

def current_memory_use(i,memory_use,memory_alt,memory_ok):
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    current_memory_usage = get_memory_usage()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º
    
    #memory_use.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    memory_use.write(f"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\n\n    ãƒ«ãƒ¼ãƒ—{i}å›ç›®:{current_memory_usage:.0f}MB")
    #print("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å®šç¾©
    MEMORY_LIMIT_MB = 2700  # 1GB
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
    if current_memory_usage > MEMORY_LIMIT_MB:
      
        memory_alt.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
        memory_ok.empty()
        #st.stop()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
    else:
        
        memory_ok.success("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")
        memory_alt.empty()
        #print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")

def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    temperature = 0.0
    #models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    #model = st.sidebar.radio("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:", models)
    model = st.sidebar.selectbox(
        "LLMå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        ["gemma2","phi4","llava-llama3","GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro"]
    )
    if model == "phi4":  
        st.session_state.model_name = "phi4"
        command = f"ollama pull {model} "
        subprocess.run(command, shell=True, capture_output=True, text=True, encoding='utf-8')
        return ChatOllama(
            temperature=temperature,
            model=st.session_state.model_name,
            #api_key= st.secrets.key.OPENAI_API_KEY,
            #streaming=True,
        )
    elif model == "gemma2":
        st.session_state.model_name = "gemma2"
        command = f"ollama pull {model} "
        subprocess.run(command, shell=True, capture_output=True, text=True, encoding='utf-8')
        return ChatOllama(
            temperature=temperature,
            model=st.session_state.model_name,
            #api_key= st.secrets.key.OPENAI_API_KEY,
            #streaming=True,
        )
    elif model == "llava-llama3":  
        st.session_state.model_name = "llava-llama3"
        command = f"ollama pull {model} "
        subprocess.run(command, shell=True, capture_output=True, text=True, encoding='utf-8')
        return ChatOllama(
            temperature=temperature,
            model=st.session_state.model_name,
            #api_key= st.secrets.key.OPENAI_API_KEY,
            #streaming=True,
        )            
    elif model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=12800,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            #model=st.session_state.model_name,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
async def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    #trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    #print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    #cleaned_response = llm_response.rstrip()
    #print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")
    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response) 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.chat_message("ai"):
        response_placeholder = st.empty()
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
        partial_text = ""
        for segment in split_response:
            if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                partial_text += segment
                response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                try:
                    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    cleaned_segment = re.sub(r'[\*#*!-]', '', segment)
                    tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                    audio_buffer = BytesIO()
                    tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                    audio_buffer.seek(0)

                    # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                    audio = AudioSegment.from_file(audio_buffer, format="mp3")
                    audio = audio._spawn(audio.raw_data, overrides={
                        "frame_rate": int(audio.frame_rate * 1.3)  # 1.5å€é€Ÿ
                    }).set_frame_rate(audio.frame_rate)
                    audio_buffer.close()

                    # éŸ³è³ªèª¿æ•´
                    audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                    audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                    audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                    #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                    low_boost = low_pass_filter(audio,1000).apply_gain(10)
                    audio = audio.overlay(low_boost)

                    # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                    output_buffer = BytesIO()
                    audio.export(output_buffer, format="mp3")
                    output_buffer.seek(0)

                    # éŸ³å£°ã®å†ç”Ÿ
                    # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                    if re.search(r"\n\n", segment):
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                        #time.sleep(1) 
                    #else:
                        #print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    audio_base64 = base64.b64encode(output_buffer.read()).decode()
                    audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
                    a=len(audio_base64)
                    #print(a)
                    # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                    audio_html = f"""
                        <audio id="audio-player" autoplay style="display:none;">
                        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                        </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)

                except Exception as e:
                    #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pass
                try:
                    time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                except Exception as e:
                  time.sleep(2) 

def trim_message_history(message_history, max_tokens=8192):  
    """  
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¶é™  
    GPT-4
    GPT-4: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4 Turbo: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4o: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude
    Claude 3 Haiku: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Sonnet: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Opus: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 2: 100,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini
    Gemini Pro: 32,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini Ultra: æœ€å¤§1,000,000ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 2/3
    Llama 2 (7B-70B): 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (8B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (70B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
    Rinna: 2,048ãƒˆãƒ¼ã‚¯ãƒ³
    ELYZA: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Nekomata: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    ãã®ä»–
    Command R+: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Mistral 7B: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Cohere: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    æ¨å¥¨ã•ã‚Œã‚‹ä¸€èˆ¬çš„ãªæˆ¦ç•¥:

    å®‰å…¨ã‚µã‚¤ã‚º: 4,000-8,000ãƒˆãƒ¼ã‚¯ãƒ³
    ãƒˆãƒªãƒŸãƒ³ã‚°é–¢æ•°ã®å®Ÿè£…
    ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®åˆ¶é™ã‚’ç¢ºèª

    """  
    total_tokens = 0  
    trimmed_history = []  
    
    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é€†é †ã«è¿½åŠ   
    for message in reversed(message_history):  
        message_tokens = len(message[1])  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã•ã‚’è¨ˆç®—  
        if total_tokens + message_tokens <= max_tokens:  
            trimmed_history.insert(0, message)  
            total_tokens += message_tokens  
        else:  
            break  
    
    return trimmed_history  

#  LLMå•ç­”é–¢æ•°
async def query_llm(user_input,frame):
    if st.session_state.model_name ==  "llava-llama3":
        user_input = " æ¬¡ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚" + user_input 
    try:
        if st.session_state.input_img == "æœ‰":    
            # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_image = cv2.imencode('.jpg', frame)[1]
            # ç”»åƒã‚’Base64ã«å¤‰æ›
            base64_image = base64.b64encode(encoded_image).decode('utf-8')  
            #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            if st.session_state.input_img == "æœ‰":
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }
                            ],
                        ),
                    ]
                )
              
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                #stream = chain.stream(user_input,base64_image)
                stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
                #print("stream=",stream)
                #response = chain.invoke(user_input)
            else:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        ("user", user_input), 
                        #(
                            #"user",
                            #[
                                #{
                                    #"type": "text",
                                    #"text": user_input
                                #}
                           # ],
                        #),
                    ]
                )
                
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            if st.session_state.output_method == "éŸ³å£°":
                response = chain.invoke({"user_input":user_input})
                #speak(response)   #st.audio ok
                await streaming_text_speak(response)
            else:    
                stream = chain.stream({"user_input":user_input})
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
                with st.chat_message('ai'):  
                    response =st.write_stream(stream) 
                           
            print(f"{st.session_state.model_name}=",response)
 
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
            st.session_state.message_history.append(("user", user_input))
            st.session_state.message_history.append(("ai", response))
            #å¤šãã®LLMã«ã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™ãŒã‚ã‚‹
            #å±¥æ­´ãŒé•·ã™ãã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒå…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã§ããªããªã‚‹
            st.session_state.message_history = trim_message_history(st.session_state.message_history)
            return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   

def qa(text_input,webrtc_ctx,cap_title,cap_image):
     # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(text_input) - len(text_input.rstrip())
    print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_text = text_input.rstrip()
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_text}'")
    with st.chat_message('user'):   
        st.write(cleaned_text) 
    
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None 
    if st.session_state.input_img == "æœ‰":
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx.video_transformer:  
            cap = webrtc_ctx.video_transformer.frame
        if cap is not None :
            #st.sidebar.header("Capture Image") 
            cap_title.header("Capture Image")     
            cap_image.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.spinner("Querying LLM..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.result= ""
        result = loop.run_until_complete(query_llm(cleaned_text,cap))
        st.session_state.result = result
    result = ""
    text_input="" 

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame

def whis_seg2(audio_segment):
    # AudioSegmentã‹ã‚‰ç›´æ¥NumPyé…åˆ—ã‚’å–å¾—
    #audio_data = np.array(audio_segment.get_array_of_samples()).astype(np.float32)
    #audio_data /= np.iinfo(audio_segment.array_type).max  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
    audio_data = np.frombuffer(audio_segment.raw_data, dtype=np.int16).astype(np.float32)
    audio_data /= np.iinfo(np.int16).max  # æ­£è¦åŒ–
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
    sample_rate = audio_segment.frame_rate
    #audio_segment = ""
    # WhisperãŒ16kHzã‚’æœŸå¾…ã™ã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å¤‰æ›
    if sample_rate != 16000:
        audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
    audio_data = whisper.pad_or_trim(audio_data)
    if not "whisper_model" in st.session_state:
        st.session_state.whisper_model = whisper.load_model("small")
    whisper_model = st.session_state.whisper_model
    #whisper_model = whisper.load_model("small")
    result = whisper_model.transcribe(audio_data, language="ja")
    #audio_data = ""
    answer2 = result['text']
    #result = ""
    return answer2

def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) ->  Tuple[str, str]:
    answer2 ="ï¼ˆä¼‘æ­¢ä¸­ï¼‰"
    # ã‚¹ãƒ†ãƒ¬ã‚ªã®å ´åˆã€ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    #print("audio_segment.channels=",audio_segment.channels)  
    if audio_segment.channels > 1:  
        audio_segment = audio_segment.set_channels(1)      
    
    if debug:
        save_audio(audio_segment, "debug_audio")
    #if st.session_state.output_whi2:
    answer2 = whis_seg2(audio_segment)
    return answer2 

async def process_audio(audio_data_bytes, sample_rate, sound_chunk):
    sound = pydub.AudioSegment(
        data=audio_data_bytes,
        sample_width=2, # audio_data_bytes.format.bytes,
        frame_rate=sample_rate,
        channels=2 , #len(audio_data_bytes.layout.channels), NG 1ï¼šæ–‡å­—åŒ–ã‘ã™ã‚‹
    )
    sound_chunk += sound
    if len(sound_chunk) > 0:
       answer2 = transcribe(sound_chunk)
    return answer2 

async def process_audio_loop_with_silence_detection(
    frames_deque_lock,
    frames_deque,
    sound_chunk,
    amp_indicator,
    ):
    """
    éŸ³å£°ã‚’ç„¡éŸ³åŒºåˆ‡ã‚Šã§ã¾ã¨ã‚ã€ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã‚’è¡Œã†ã€‚
    """
    audio_buffer = []
    last_sound_time = time.time()
    silence_detected = False

    while True:
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
        with frames_deque_lock:
            while len(frames_deque) > 0:
                frame = frames_deque.popleft() # å·¦ç«¯ã‹ã‚‰è¦ç´ ã‚’å–ã‚Šå‡ºã—ã¦å‰Šé™¤
                audio_chunk = frame.to_ndarray().astype(np.int16)
                audio_buffer.append(audio_chunk)
                st.session_state.frame_sample_rate = frame.sample_rate
                amp=np.max(np.abs(audio_chunk)) 
                #st.session_state.amp = amp
                amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
                #print(f"éŸ³å£°æŒ¯å¹…/ç„¡éŸ³é–¾å€¤={amp}/{SILENCE_THRESHOLD}")
                #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
                if not amp < st.session_state.amp_threshold:
                    last_sound_time = time.time()
                    silence_detected = False
                else:
                    silence_detected = time.time() - last_sound_time >= st.session_state.silence_threshold
        #print(f"ç„¡éŸ³åˆ¤å®š={silence_detected}")
        #print(f"audio_buffer={len(audio_buffer)}")
        # ç„¡éŸ³åŒºåˆ‡ã‚ŠãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        if silence_detected and audio_buffer:
            audio_data = np.concatenate(audio_buffer).tobytes()
            try:
                answer2 = await process_audio(audio_data, st.session_state.frame_sample_rate, sound_chunk)
                ##########################################################
                #text_output.write(f"èªè­˜çµæœ: {answer}")
                #ãŠã‹ã—ãªå›ç­”ã‚’é™¤å»
                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
                phrases = (
                    "ã‚ã‚ŠãŒã¨ã†", 
                    "ãŠç–²ã‚Œæ§˜", "ã‚“ã‚“ã‚“ã‚“ã‚“ã‚“", 
                    "by H.","ã‚¹ã‚¿ãƒƒãƒ•ã•ã‚“ã®ãŠè©±ã‚’",
                    "ã„ã„ãˆ- ã„ã„ãˆ- ã„ã„ãˆ-",
                    "ã”ã¡ãã†ã•ã¾ã§ã—ãŸ"
                    )
                if len(answer2) < 5:
                    pass
                elif any(phrase in answer2 for phrase in phrases):
                    pass
                else:
                    #with text_output.chat_message('user'):
                        #st.write(answer2)
                    print("[Whis_seg]",answer2) 
                    return answer2 
                audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                silence_detected = False
            except Exception as e:
                st.error(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={st.session_state.amp}")
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
        # å‡¦ç†è² è·ã‚’æŠ‘ãˆã‚‹ãŸã‚ã«çŸ­ã„é…å»¶ã‚’æŒ¿å…¥
        time.sleep(0.1)


def app_sst_with_video():
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])
    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)
        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)
        return new_frames
        
     # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    if "streaming" not in st.session_state:
        st.session_state["streaming"] = True  # åˆæœŸçŠ¶æ…‹ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿä¸­
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=st.session_state["streaming"], 
            mode=WebRtcMode.SENDRECV, #.SENDONLY,  #
            #audio_receiver_size=2048,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            #å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  
        )
    if not webrtc_ctx.state.playing:
        return
    #status_indicator.write("Loading...")
    cap_title = st.sidebar.empty()    
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 
    
    text_input = ""
    st.sidebar.title("Options")
    col1, col2 ,col3= st.sidebar.columns(3)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
   
    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    if st.session_state.input_method == "éŸ³å£°":
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]  # ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–,ç”»é¢ã‚¯ãƒªã‚¢
        status_indicator = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        status_indicator.write("ğŸ¤–æº–å‚™ä¸­ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
        status_indicator = st.sidebar.empty()    
        amp_indicator = st.sidebar.empty()
        st.session_state.amp_threshold = st.sidebar.slider(
            "ç„¡éŸ³æŒ¯å¹…é–¾å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000):",
            min_value=300, max_value=3000, value=1000, step=100
            )
        st.session_state.silence_threshold = st.sidebar.slider(
            "ç„¡éŸ³æœ€å°æ™‚é–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ç§’ï¼‰",
            min_value=0.1, max_value=3.0, value=0.5, step=0.1
            )
        if not "whisper_model" in st.session_state:
            st.session_state.whisper_model = whisper.load_model("small") #,device = "cuda")
        #base:74M,small:244M,medium:769M,large:1550M 
        #st.session_state["streaming"] = False  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœæ­¢
        #status_indicator.empty  #æº–å‚™ãŒã§ããŸã®ã§ã€"ğŸ¤–æº–å‚™ä¸­ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")ã‚’æ¶ˆã™ NG

        memory_use = st.sidebar.empty()
        memory_alt = st.sidebar.empty()
        memory_ok = st.sidebar.empty()
        #for key, label, default in [
            #("output_whi2", "Whis-segmå‡ºåŠ›ï¼ˆç„¡æ–™ï¼‰", True),
            #]:
            #st.session_state[key] = st.sidebar.toggle(label, value=default)

        i = 0
        #current_memory_use(i,memory_use,memory_alt,memory_ok)
        frames_deque_lock = threading.Lock()
        # frames_deque_lockã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™ãŒã€
        # dequeã®ã‚¯ãƒªã‚¢æ“ä½œãªã©ã§ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆãŒèµ·ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        # dequeã®æœ€å¤§é•·ã‚’è¨­å®šï¼ˆä¾‹: deque([], maxlen=100)) ã—ã€ãƒãƒƒãƒ•ã‚¡æº¢ã‚Œã‚’é˜²æ­¢ã™ã‚‹æ–¹ãŒå®‰å…¨ã§ã™ã€‚
        frames_deque: deque = deque([], maxlen=100) #NG 1
        
        sound_chunk = pydub.AudioSegment.empty()  
        while True:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
            current_memory_use(i,memory_use,memory_alt,memory_ok)
            mem_use = get_memory_usage()
            i += 1
            #print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={i}å›ç›®:{mem_use}")
            st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
            status_indicator.write(f"len(frames_deque)={len(frames_deque)}")
            # éŸ³å£°å‡¦ç†ã®éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’èµ·å‹•
            text_input = asyncio.run(process_audio_loop_with_silence_detection(
                frames_deque_lock,
                frames_deque,
                sound_chunk,
                amp_indicator,
                ))
            qa(text_input,webrtc_ctx,cap_title,cap_image)
            text_input = ""
    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]  # ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–,ç”»é¢ã‚¯ãƒªã‚¢
        st.session_state["streaming"] = True  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿ
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"):
                button_input = "æ—¥æœ¬ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("å°æ¾å¸‚ã®ãŠã„ã—ã„æ–™ç†åº—ã¯ï¼Ÿ"):
                button_input = "å°æ¾å¸‚ã®ãŠã„ã—ã„æ–™ç†åº—ã¯ï¼Ÿ"
        with col8:
            if st.button("Webç”»é¢ã§ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã‚ªã‚»ãƒ­ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦"):
                button_input = "Webç”»é¢ã§ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã‚ªã‚»ãƒ­ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦"
        if button_input !="":
            st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if button_input:
            text_input = button_input
        if text_input:
            qa(text_input,webrtc_ctx,cap_title,cap_image)


def init_page():
    st.set_page_config(
        page_title="Yas Chatbot",
        page_icon="ğŸ¤–"
    )
    st.header("Yas Chatbot(ç”»åƒã€éŸ³å£°å¯¾å¿œ) ğŸ¤–")
    st.write("""Webã‚«ãƒ¡ãƒ©ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›ã€éŸ³å£°ã§ã®å…¥å‡ºåŠ›ãŒã§ãã¾ã™ã€‚\n
             ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã¦ä½¿ç”¨ã€‚""") 
    
def init_messages():
    clear_button = st.sidebar.button("ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]   

def main():
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
    init_messages()
    
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = "" 
    
    app_sst_with_video() 
     
###################################################################      
if __name__ == "__main__":
    main()

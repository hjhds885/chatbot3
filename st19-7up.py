from pydoc import text
import threading
import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase

#import speech_recognition as sr
from pydub import AudioSegment
import queue, pydub, tempfile
import whisper
import torch
import torchaudio
import torchvision

from typing import List
#import io
import wave
import asyncio
import time

import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
#from langchain_ollama import ChatOllama
from langchain_ollama.chat_models import ChatOllama
import base64
#import keyboard
from gtts import gTTS
import os
import re
from pydub.effects import low_pass_filter, high_pass_filter
from io import BytesIO
import psutil
import gc

# é–¢æ•°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
def get_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§è¿”ã™
    return mem_info.rss / (1024 * 1024)

def current_memory_use(memory_use,memory_alt,memory_ok):
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    current_memory_usage = get_memory_usage()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º
    
    memory_use.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    #print("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å®šç¾©
    MEMORY_LIMIT_MB = 1024*3  # 1GB
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
    if current_memory_usage > MEMORY_LIMIT_MB:
      
        memory_alt.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
        #st.stop()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
    else:
        
        memory_ok.success("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")
        #print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")


def init_page():
    st.set_page_config(
        page_title="Mr.Yas Chatbot",
        page_icon="ğŸ¤–"
    )
    st.header("Mr.Yas Chatbot ğŸ¤–")
    st.write("""Safari,Chrome,Firefoxãªã©Webãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹è¨­å®šã«ã—ã¦ãã ã•ã„ã€‚
         support.apple.com,support.google.com,support.mozilla.orgå‚ç…§ã€‚""") 
   
def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ] 

def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    #temperature = st.sidebar.slider(
        #"Temperature(å›ç­”ãƒãƒ©ãƒ„ã‚­åº¦åˆ):", min_value=0.0, max_value=2.0, value=0.0, step=0.01)
    temperature = 0.0   
    models = ("llava-llama3","GPT-4o","Claude 3.5 Sonnet","Gemini 1.5 Pro")
    model = st.sidebar.radio("Choose a modelï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼‰:", models)
    
    if model == "llava-llama3":  
        st.session_state.model_name = "llava-llama3"
        return ChatOllama(
            temperature=temperature,
            model=st.session_state.model_name,
            #api_key= st.secrets.key.OPENAI_API_KEY,
            #streaming=True,
        ) 
       
    elif model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=512,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            #model=st.session_state.model_name,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
def speak(text):
    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    cleaned_text = re.sub(r'\*!?', '', text)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›
    tts = gTTS(text=cleaned_text, lang='ja')

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
        tts.save(temp_file.name)
        temp_file_path = temp_file.name

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
    audio = AudioSegment.from_file(temp_file_path)
    # é€Ÿåº¦ã‚’1.5å€ã«ã™ã‚‹ï¼ˆ2.0ã«ã™ã‚‹ã¨2å€é€Ÿï¼‰
    faster_audio = audio.speedup(playback_speed=1.5)

    # ã‚‚ã†ä¸€ã¤ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as faster_temp_file:
        faster_audio.export(faster_temp_file.name, format="mp3")
        faster_temp_file_path = faster_temp_file.name

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›
    with open(faster_temp_file_path, "rb") as audio_file:
        audio_bytes = audio_file.read()
        st.audio(audio_bytes, format="audio/mp3", start_time=0, autoplay=True)

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(temp_file_path)
    os.remove(faster_temp_file_path)

def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")

    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_response = llm_response.rstrip()
    print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")



    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    #split_response = re.split(r'([ã€ã€‚ ]+)', llm_response)  #å…ƒ
    #split_response = re.split(r'(:;=!?[ã€ã€‚ ]+)', llm_response)
    #split_response = re.split(r'([123456789\n-;=:ã€ã€‚ ]ã€‚+)', llm_response)
    #split_response = re.split(r'([ã€‚123456789\n-;=:ã€ã€‚ ]+)', llm_response) 
    split_response = re.split(r'([\r\n-;=:ã€ã€‚ ]+)', llm_response) 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.chat_message("ai"):
        response_placeholder = st.empty()
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
        partial_text = ""
        
        for segment in split_response:
            if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                partial_text += segment
                response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º

                # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                try:
                    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    cleaned_segment = re.sub(r'[\*!-]', '', segment)
                    #audio_buffer = BytesIO()  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ç”¨ãƒãƒƒãƒ•ã‚¡ã‚’åˆæœŸåŒ–
                    #tts = gTTS(partial_text, lang="ja")
                    tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                    audio_buffer = BytesIO()
                    tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                    audio_buffer.seek(0)

                    # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                    audio = AudioSegment.from_file(audio_buffer, format="mp3")
                    audio = audio._spawn(audio.raw_data, overrides={
                        "frame_rate": int(audio.frame_rate * 1.2)  # 1.5å€é€Ÿ
                    }).set_frame_rate(audio.frame_rate)
                    audio_buffer.close()

                    # éŸ³è³ªèª¿æ•´
                    audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                    audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                    audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                    #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                    low_boost = low_pass_filter(audio,1000).apply_gain(10)
                    audio = audio.overlay(low_boost)

                    # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                    output_buffer = BytesIO()
                    audio.export(output_buffer, format="mp3")
                    output_buffer.seek(0)

                    # éŸ³å£°ã®å†ç”Ÿ
                    # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                    #patterns = ["\n\n1.", "\n\n2.","\n\n3.", "\n\n4.""\n\n5.", "\n\n6.", "\n\n7.","\n\n8.", "\n\n9.""\n\n10.", "\n\n11."]
                    if re.search(r"\n\n", segment):
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                        time.sleep(2) 
                    else:
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

                    #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    audio_base64 = base64.b64encode(output_buffer.read()).decode()
                    audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º

                    a=len(audio_base64)
                    #print(a)
               
                    # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                    audio_html = f"""
                   <audio id="audio-player" autoplay style="display:none;">
                        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                    </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)

                except Exception as e:
                    #st.error(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pass
                try:
                    time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                except Exception as e:
                  time.sleep(2) 


def trim_message_history(message_history, max_tokens=8192):  
    """  
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¶é™  
    GPT-4
    GPT-4: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4 Turbo: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4o: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude
    Claude 3 Haiku: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Sonnet: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Opus: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 2: 100,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini
    Gemini Pro: 32,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini Ultra: æœ€å¤§1,000,000ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 2/3
    Llama 2 (7B-70B): 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (8B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (70B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
    Rinna: 2,048ãƒˆãƒ¼ã‚¯ãƒ³
    ELYZA: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Nekomata: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    ãã®ä»–
    Command R+: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Mistral 7B: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Cohere: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    æ¨å¥¨ã•ã‚Œã‚‹ä¸€èˆ¬çš„ãªæˆ¦ç•¥:

    å®‰å…¨ã‚µã‚¤ã‚º: 4,000-8,000ãƒˆãƒ¼ã‚¯ãƒ³
    ãƒˆãƒªãƒŸãƒ³ã‚°é–¢æ•°ã®å®Ÿè£…
    ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®åˆ¶é™ã‚’ç¢ºèª

    """  
    total_tokens = 0  
    trimmed_history = []  
    
    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é€†é †ã«è¿½åŠ   
    for message in reversed(message_history):  
        message_tokens = len(message[1])  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã•ã‚’è¨ˆç®—  
        if total_tokens + message_tokens <= max_tokens:  
            trimmed_history.insert(0, message)  
            total_tokens += message_tokens  
        else:  
            break  
    
    return trimmed_history  

#  LLMå•ç­”é–¢æ•°   
async def query_llm(user_input,frame):
    #print("user_input=",user_input)
    if st.session_state.model_name ==  "llava-llama3":
        user_input = " æ¬¡ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚" + user_input 
    try:
        if st.session_state.input_img == "æœ‰":    
            # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_image = cv2.imencode('.jpg', frame)[1]
            # ç”»åƒã‚’Base64ã«å¤‰æ›
            base64_image = base64.b64encode(encoded_image).decode('utf-8')  
            #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "keep_gpt-4o":
            llm = st.session_state.llm  
            stream = llm.stream([
                    *st.session_state.message_history,
                    (
                        "user",
                        [
                            {
                                "type": "text",
                                "text": user_input
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "auto"
                                },
                            }
                        ]
                    )
                ])
            #response = chain.invoke(user_input)
           
        
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            
        elif st.session_state.model_name ==  "keep_command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            prompt = ChatPromptTemplate.from_messages([
                    *st.session_state.message_history,
                    ("user", "{user_input}")  # ã“ã“ã«ã‚ã¨ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒå…¥ã‚‹
                ])
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            stream = chain.stream(user_input)
            
            #response = chain.invoke(user_input)
            
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            if st.session_state.input_img == "æœ‰":
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }
                            ],
                        ),
                    ]
                )
              
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                #stream = chain.stream(user_input,base64_image)
                stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
                #print("stream=",stream)
                #response = chain.invoke(user_input)
            else:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                }
                            ],
                        ),
                    ]
                )
                
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            if st.session_state.output_method == "éŸ³å£°":
                response = chain.invoke({"user_input":user_input})
            else:    
                stream = chain.stream({"user_input":user_input})
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
                with st.chat_message('ai'):  
                    response =st.write_stream(stream) 

                            
            print(f"{st.session_state.model_name}=",response)

            # éŸ³å£°å‡ºåŠ›å‡¦ç†                
            if st.session_state.output_method == "éŸ³å£°":
                #speak(response)   #st.audio ok
                streaming_text_speak(response)

            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
            st.session_state.message_history.append(("user", user_input))
            st.session_state.message_history.append(("ai", response))
            #å¤šãã®LLMã«ã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™ãŒã‚ã‚‹
            #å±¥æ­´ãŒé•·ã™ãã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒå…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã§ããªããªã‚‹
            st.session_state.message_history = trim_message_history(st.session_state.message_history)
            return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   

class VideoTransformer(VideoTransformerBase):
        def __init__(self):
            self.frame = None

        def recv(self, frame):   
            self.frame = frame.to_ndarray(format="bgr24")
            return frame

###########################################################################    
def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    """
    OpenAIã®Whisper ASRã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_segment (AudioSegment): æ–‡å­—èµ·ã“ã—ã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã€‚
        debug (bool): Trueã®å ´åˆã€ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
    æˆ»ã‚Šå€¤:
        str: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    if debug:
        save_audio(audio_segment, "debug_audio")
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
        audio = whisper.load_audio(tmpfile.name)
        audio = whisper.pad_or_trim(audio)
        # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        if not "whisper_model" in st.session_state:
            st.session_state.whisper_model = whisper.load_model("base")
            #base:74M,small:244M,medium,large
        # ãƒ«ãƒ¼ãƒ—å†…ã§å†åˆ©ç”¨
        whisper_model = st.session_state.whisper_model
        #model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
        result = whisper_model.transcribe(audio, language="ja") 
        #base:74M,small:244M,medium,large
        # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        #result = model.transcribe(audio, language="ja")  # æ—¥æœ¬èªã‚’æŒ‡å®š
        answer = result['text']
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
        if len(answer) < 5 or "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
            #print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
            return None
        #elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
            #return None 
        
    tmpfile.close()  
    os.remove(tmpfile.name)
    print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
    st.session_state.text_output = answer
    return answer
###############################################################
def frame_energy(audio_frame):
    try:
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã¿
        samples = np.frombuffer(audio_frame.to_ndarray().tobytes(), dtype=np.int16)

        # é…åˆ—ã®é•·ã•ãŒ0ã®å ´åˆã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’0ã¨ã—ã¦è¿”ã™
        if len(samples) == 0:
            return 0.0

        # äºŒä¹—å¹³å‡å¹³æ–¹æ ¹ï¼ˆRMSï¼‰ã‚’è¨ˆç®—ã—ã¦ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¿”ã™
        rms_energy = np.sqrt(np.mean(samples.astype(np.float64) ** 2))
        
        #ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ16ãƒ“ãƒƒãƒˆPCMã®æœ€å¤§å€¤ã§æ­£è¦åŒ–ï¼‰
        #ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¨ãƒãƒ«ã‚®ãƒ¼å€¤ã¯ç›¸å¯¾çš„ãªéŸ³é‡ã‚’ç¤ºã—ã¾ã™ã€‚
        # 0.0 ã«è¿‘ã„å€¤ã¯ç„¡éŸ³ã€1.0 ã«è¿‘ã„å€¤ã¯æœ€å¤§éŸ³é‡ã«ç›¸å½“ã—ã¾ã™ã€‚
        max_amplitude = 32767.0  # 16ãƒ“ãƒƒãƒˆPCMã®æœ€å¤§æŒ¯å¹…
        scaled_energy = rms_energy / max_amplitude
        
        return scaled_energy
    except Exception as e:
        print(f"Error calculating frame energy: {e}")
        return 0.0
##################################################################
def frame_amplitude(audio_frame):
    samples = np.frombuffer(audio_frame.to_ndarray().tobytes(), dtype=np.int16)
    max_amplitude = np.max(np.abs(samples))
    #print("max_amplitude=",max_amplitude)
    return max_amplitude 

def process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold, amp_threshold):
    """
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †æ¬¡å‡¦ç†ã—ã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ã§ã™ã€‚
    ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸€å®šæ•°ä»¥ä¸Šç¶šã„ãŸå ´åˆã€ç„¡éŸ³åŒºé–“ã¨ã—ã¦å‡¦ç†ã—ã€å¾Œç¶šã®å‡¦ç†ï¼ˆä¾‹ãˆã°ã€éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã«å½¹ç«‹ã¦ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€ç„¡éŸ³ã‚„éŸ³å£°ã®æœ‰ç„¡ã‚’æ­£ç¢ºã«æ¤œå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frames (list[VideoTransformerBase.Frame]): å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        energy_threshold (int): ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚
        amp_threshold:ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        
    """
    for audio_frame in audio_frames:
        a0 = audio_frame
        sound_chunk = add_frame_to_chunk(a0, sound_chunk)
        energy = frame_energy(a0)
        amplitude = frame_amplitude(a0)
        
        if energy < energy_threshold or amplitude < amp_threshold:
            silence_frames += 1 
            #ç„¡éŸ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’1ã¤å¢—ã‚„ã—ã¾ã™ã€‚
            #print(f"ç„¡éŸ³ãƒ¬ãƒ™ãƒ«ã®éŸ³é‡={'{:.03f}'.format(energy)},æŒ¯å¹…={amplitude}")
        else:
            silence_frames = 0 
            print(f"éŸ³å£°ãƒ¬ãƒ™ãƒ«ã®éŸ³é‡={'{:.03f}'.format(energy)},æŒ¯å¹…={amplitude}")
            #print(f"éŸ³é‡={energy}")  #nanã§ã‚‚å•é¡Œãªã—ã€å¤§å¤‰å¤§ããªå€¤32767.0â‡’Max1.0ã«è¦æ ¼åŒ–ã—ãŸ
            #ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦0ã«ã—ã¾ã™ã€‚
            #print(f"éŸ³é‡={'{:.03f}'.format(energy)},éŸ³å£°æŒ¯å¹…={amplitude},ãƒ•ãƒ¬ãƒ¼ãƒ æ•°={len(audio_frames)}")  #é‡è¦
            #print(f"éŸ³é‡={energy},éŸ³å£°æŒ¯å¹…={amplitude},ãƒ•ãƒ¬ãƒ¼ãƒ æ•°={len(audio_frames)}")  #é‡è¦
    return sound_chunk, silence_frames,energy,amplitude

def add_frame_to_chunk(audio_frame, sound_chunk):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frame (VideoTransformerBase.Frame): è¿½åŠ ã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    æˆ»ã‚Šå€¤ï¼š
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
   
    """
    sound = pydub.AudioSegment(
        data=audio_frame.to_ndarray().tobytes(),
        sample_width=audio_frame.format.bytes,
        frame_rate=audio_frame.sample_rate,
        channels=len(audio_frame.layout.channels),
    )
    sound_chunk += sound
    return sound_chunk

def handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³ã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        silence_frames_threshold (int): ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã—ãã„å€¤ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
   
    """
    if silence_frames >= silence_frames_threshold: 
        #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ100ä»¥ä¸Šã®æ™‚ã€éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆé–“éš”ï¼‰ã¨ã—ã¦æ‰±ã†
        if len(sound_chunk) > 0:
            #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒé€£ç¶šã—ãŸã‚‰ã€éŸ³å£°ã®é€”åˆ‡ã‚Œã¨ã—ã¦ã€ãã“ã¾ã§ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã„ã‚‹
            text = transcribe(sound_chunk)
            #text_output.write(text)
            #print("handle_silenceãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
            #print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³æ™‚ã®å¿œç­”=",text)

            sound_chunk = pydub.AudioSegment.empty()
            silence_frames = 0

    return sound_chunk, silence_frames

def handle_queue_empty(sound_chunk, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    å¼•æ•°:
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤:
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    """
    if len(sound_chunk) > 0:
        text = transcribe(sound_chunk)
        text_output.write(text)
        #print("handle_queue_emptyãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
        #st.session_state.text_output = text
        sound_chunk = pydub.AudioSegment.empty()

    return sound_chunk

def app_sst_with_video():
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã®ãŸã‚ã®ä¸»ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã€‚
        ã“ã®æ©Ÿèƒ½ã¯ã€WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å—ä¿¡ã‚’é–‹å§‹ã—ã€éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã€
        ä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹é™å¯‚ãŒç¶šã„ãŸã¨ãã«éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_receiver_size:å‡¦ç†éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ512
            å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
        status_indicator: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå®Ÿè¡Œä¸­ã¾ãŸã¯åœæ­¢ä¸­ï¼‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        text_output: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        timeout (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå—ä¿¡æ©Ÿã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ç§’ã€‚
        energy_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ•ãƒ¬ãƒ¼ãƒ ãŒé™å¯‚ã¨è¦‹ãªã•ã‚Œã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é–¾å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000ã€‚
        silence_frames_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ–‡å­—èµ·ã“ã—ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®é€£ç¶šã™ã‚‹é™å¯‚ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯100ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    memory_use = st.sidebar.empty()
    memory_alt = st.sidebar.empty()
    memory_ok = st.sidebar.empty()
    current_memory_use(memory_use,memory_alt,memory_ok)
        
    text_input = ""
    audio_receiver_size =2048
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx1 = webrtc_streamer(
            key="example",
            desired_playing_state=True, 
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": False},
            video_processor_factory=VideoTransformer,
            )
    #st.sidebar.header("Capture Image") 
    cap_title = st.sidebar.empty()    
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 
    status_indicator = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
    st.sidebar.title("Options")
    init_messages()
    text_output = st.empty()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = "" 
    st.session_state.text_output = ""

    col1, col2 ,col3= st.sidebar.columns(3)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º 
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
      
    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    #print("Before_st.session_state.input_method=",st.session_state.input_method)
    if st.session_state.input_method == "éŸ³å£°": 
        audio_receiver_size = st.sidebar.slider(
        "éŸ³å£°å—ä¿¡å®¹é‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1024:", 
        min_value=512, max_value=4096, value=1024, step=512
        )
        # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®é–¾å€¤    
        energy_threshold = st.sidebar.slider(
        "ç„¡éŸ³ã®æœ€å¤§éŸ³é‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.14):", 
        min_value=0.010, max_value=0.300, value=0.14, step=0.01
        )
        amp_threshold = st.sidebar.slider(
            "ç„¡éŸ³ã®æœ€å¤§æŒ¯å¹…ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ9000):", 
            min_value=1000, max_value=100000, value=9000, step=1000
            )
        silence_frames_threshold = st.sidebar.slider(
            "é€£ç¶šç„¡éŸ³åŒºé–“ï¼ˆéŸ³å£°åŒºåˆ‡ã‚Šæ™‚é–“ x10msï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ï¼ˆ=1ç§’):", 
            min_value=0, max_value=200, value=100, step=10
            )
        timeout = st.sidebar.slider(
            "timeout(ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ç§’):", 
            min_value=1, max_value=3, value=1, step=1
            )
        
        with st.sidebar:
            webrtc_ctx = webrtc_streamer(
                key="speech-to-text and video",
                desired_playing_state=True, 
                mode=WebRtcMode.SENDONLY,  #SENDRECV, #.
                audio_receiver_size=audio_receiver_size,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
                #queued_audio_frames_callback=queued_audio_frames_callback,
                rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
                media_stream_constraints={"video": False, "audio": True},
                video_processor_factory=VideoTransformer,
                )
            
        if not webrtc_ctx.state.playing:
            return
        #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
        st.session_state.energy_threshold = energy_threshold
        st.session_state.amp_threshold = amp_threshold
        st.session_state.silence_frames_threshold = silence_frames_threshold
        st.session_state.timeout = timeout

        sound_chunk = pydub.AudioSegment.empty()
        silence_frames = 0

        st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
        status_indicator.write("éŸ³å£°èªè­˜å‹•ä½œä¸­...")
        text_output = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        i=0
        while True:
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
            current_memory_use(memory_use,memory_alt,memory_ok)
            # deque ã®æœ€å¤§é•·ã‚’è¨­å®š
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
            mem_use= get_memory_usage() 
            i = i + 1
            print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={i}å›ç›®:{mem_use}")

            
            if webrtc_ctx.audio_receiver:
                timeout=st.session_state.timeout
                energy_threshold=st.session_state.energy_threshold
                amp_threshold=st.session_state.amp_threshold
                silence_frames_threshold= st.session_state.silence_frames_threshold    

                try:
                    audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=timeout)
                  
                except queue.Empty:
                    status_indicator.write("No frame arrived.")
                    sound_chunk = handle_queue_empty(sound_chunk, text_output)
                    continue
                sound_chunk, silence_frames ,energy,amplitude= process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold,amp_threshold)
                sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output)
                
                status_indicator.write(f"éŸ³å£°ãƒ¬ãƒ™ãƒ«:\n energy={'{:.03f}'.format(energy)}/threshold={energy_threshold},\n amplitude={amplitude}/threshold={amp_threshold}")
            else:    
                status_indicator.write("éŸ³å£°èªè­˜åœæ­¢")

            if len(st.session_state.text_output) > 4 :
                print("st.session_state.text_output=",st.session_state.text_output)    
                #st.session_state.user_input=st.session_state.text_output
                #with st.chat_message('user'):  
                    #st.write(st.session_state.user_input)
                #text_input =  st.session_state.user_input
                text_input =  st.session_state.text_output 
                st.session_state.text_output = ""
                #st.session_state.user_input =""
            #ã“ã‚Œä»¥é™ã¯ã€éŸ³å£°å…¥åŠ›ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›å…±é€šã®å‡¦ç†ã¸
            if text_input: 
                #with st.chat_message('user'):   
                    #st.write(text_input) 
                qa(text_input,webrtc_ctx1,cap_title,cap_image)
                st.write(f"ğŸ¤–ä½•ã‹è©±ã—ã¦! ...energy={'{:.03f}'.format(energy)}/threshold={energy_threshold}, amplitude={amplitude}/threshold={amp_threshold}")
                text_input = ""
                
    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
        current_memory_use(memory_use,memory_alt,memory_ok)
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"):
                button_input = "æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("å°æ¾å¸‚ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"):
                button_input = "å°æ¾å¸‚ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        with col8:
            if st.button("ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"):
                button_input = "ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"
        #if button_input !="":
            #st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if button_input:
            text_input = button_input
        if text_input:
            qa(text_input,webrtc_ctx1,cap_title,cap_image)
    ###################################################################################
def qa(text_input,webrtc_ctx1,cap_title,cap_image): 
    with st.chat_message('user'):   
        st.write(text_input) 
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None 
    if st.session_state.input_img == "æœ‰":
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx1.video_transformer:  
            cap = webrtc_ctx1.video_transformer.frame
        if cap is not None :
            #st.sidebar.header("Capture Image") 
            cap_title.header("Capture Image")     
            cap_image.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.spinner("Querying LLM..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.result= ""
        result = loop.run_until_complete(query_llm(text_input,cap))
        st.session_state.result = result
    result = ""
    text_input=""

################################################################### 
def main():
    
    #ç”»é¢è¡¨ç¤º
    init_page()
    
    #init_messages()
    
    app_sst_with_video()  
###################################################################      
if __name__ == "__main__":
    main()
